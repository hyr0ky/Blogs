#LLMAttackttack
#AIcybercurity 


## 概述
>训练数据污染（`Training Data Poisoning`）指的是对预训练数据或在微调或嵌入过程中涉及的数据进行操纵，以引入可能危害模型安全性、效果或道德行为的漏洞（这些漏洞都具有独特且有时共享的攻击向量）、后门或偏见。污染的信息可能会被呈现给用户，或者造成其他风险，如性能下降、下游软件滥用和声誉损害。即使用户不信任有问题的AI输出，风险仍然存在，包括降低模型能力和对品牌声誉的潜在危害。

- 预训练数据是指根据任务或数据集对模型进行训练的过程
- 微调涉及采用已经训练过的现有模型，并通过使用策划的数据集对其进行训练，以适应更窄的主题或更专注的目标。这个数据集通常包括输入示例和相应的期望输出
- 嵌入过程是将分类数据（通常是文本）转换为可以用于训练语言模型的数字表示的过程。嵌入过程涉及将文本数据中的单词或短语表示为连续向量空间中的向量，这些向量通常将文本数据输入到已经在大量文本语料库上进行训练的神经网络中生成的

数据污染被视为完整性攻击，因为篡改训练数据会影响模型输出正确的预测能力

## 漏洞示例

1. 恶意行为或者竞争对手故意创建不准确或恶意文件，针对模型的预训练、微调数据或嵌入。考虑[分割视图数据污染](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%201%20Split-View%20Data%20Poisoning.jpeg)和[前沿数据污染](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%202%20Frontrunning%20Data%20Poisoning.jpeg)攻击向量进行说明。
	1. 受害模型使用伪造信息进行训练，这在生成AI提示的输出中反映出来，呈现给消费者
2. 恶意行为者能够直接注入伪造，有偏见或有害内容到模型的训练过程中，然后在随后的输出中返回
3. 一个毫不知情的用户间接地将敏感或专有的数据注入到模型的训练过程中,然后在随后的输出中返回
4. 模型使用未经验证的数据进行训练，源、起源或训练阶段示例中的内容未经验证，如果数据被污染或不正确，则可能导致错误的结果。
5. 无限制的基础架构访问或不足的沙箱可能会允许模型摄取不安全的训练数据，导致有偏见或有害的输出。这个例子也存在于训练阶段的任何示例中。
	1. 在这种情况下，用户对模型的输入可能会反映在向另一个用户的输出中（导致违规），或者LLM的用户可能会收到与所摄取的数据类型相比不准确、不相关或有害的输出（通常在模型卡中反映出来）。



## 攻击场景

- LLM生成AI提示输出可能会误导应用程序的用户，导致有偏见的观点、跟踪或甚至更糟的情况，例如仇恨犯罪等。
- 如果训练数据没有正确过滤和消毒，应用程序的恶意用户可能会尝试影响并向模型注入有毒数据，以使其适应有偏见和虚假数据。
- 恶意行为者或竞争对手故意创建不准确或恶意文件，针对模型的训练数据，同时根据输入训练模型。受害模型使用这些伪造信息进行训练，这在生成AI提示的输出中反映出来，呈现给其消费者。
- 如果在LLM应用程序的客户端输入用于训练模型时未执行足够的消毒和过滤，那么漏洞Prompt Injection可能会成为这种漏洞的攻击向量。即，如果从客户端输入模型的恶意或伪造数据作为提示注入技术的一部分，则这可能会被直接反映到模型数据中。



## Reference
- [**CS324 - Large Language Models**](https://stanford-cs324.github.io/winter2022/lectures/data/): Stanford University
- [**How data poisoning attacks corrupt machine learning models**](https://www.csoonline.com/article/3613932/how-data-poisoning-attacks-corrupt-machine-learning-models.html): CSO Online
- [**Tay Poisoning**](https://atlas.mitre.org/studies/AML.CS0009/): MITRE ATLAS
- [**PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news**](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/): Mithril Security
- [**Inject My PDF: Prompt Injection for your Resume**](https://kai-greshake.de/posts/inject-my-pdf/): Kai Greshake
- [**Backdoor Attacks on Language Models**](https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f): Medium
- [**Poisoning Language Models During Instruction**](https://arxiv.org/abs/2305.00944): Cornell University
- [**FedMLSecurity**](https://arxiv.org/abs/2306.04959): Cornell University
- [**The poisoning of ChatGPT**](https://softwarecrisis.dev/letters/the-poisoning-of-chatgpt/): Out of the Software Crisis
- [**Poisoning Web-Scale Training Datasets**](https://www.youtube.com/watch?v=h9jf1ikcGyk): Nicholas Carlini | Stanford MLSys #75
- [**OWASP CycloneDX v1.5**](https://cyclonedx.org/capabilities/mlbom/): OWASP CycloneDX